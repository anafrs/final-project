{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.functions import clean_up, tokenize, stem_and_lemmatize, lemmatize,  remove_stopwords\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pprint\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagram = pd.read_csv(\"instagram.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Analysis - experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired here https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is the process of identifying topics in a set of documents.Latent Dirichlet Allocation (LDA) is a probabilistic method for Topic Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['best',\n",
       "  'offense',\n",
       "  'good',\n",
       "  'defense',\n",
       "  'healthy',\n",
       "  'tip',\n",
       "  'boast',\n",
       "  'immunity',\n",
       "  'work',\n",
       "  'covid',\n",
       "  'scare',\n",
       "  'scared',\n",
       "  'take',\n",
       "  'care'],\n",
       " ['nature',\n",
       "  'reflects',\n",
       "  'back',\n",
       "  'u',\n",
       "  'imbalance',\n",
       "  'health',\n",
       "  'earth',\n",
       "  'ecosystem',\n",
       "  'compromised',\n",
       "  'health',\n",
       "  'also',\n",
       "  'compromised',\n",
       "  'part',\n",
       "  'nature',\n",
       "  'sickness',\n",
       "  'becomes',\n",
       "  'sickness',\n",
       "  'health',\n",
       "  'health',\n",
       "  'art',\n",
       "  'mimbirose'],\n",
       " ['concerning',\n",
       "  'current',\n",
       "  'outbreak',\n",
       "  'covid',\n",
       "  'coronavirus',\n",
       "  'ha',\n",
       "  'declared',\n",
       "  'pandemic',\n",
       "  'must',\n",
       "  'take',\n",
       "  'precaution',\n",
       "  'safety',\n",
       "  'measure',\n",
       "  'avoid',\n",
       "  'spread',\n",
       "  'even',\n",
       "  'simple',\n",
       "  'handshake',\n",
       "  'cause',\n",
       "  'transfer',\n",
       "  'harmful',\n",
       "  'disease',\n",
       "  'causing',\n",
       "  'bacteria',\n",
       "  'safe',\n",
       "  'way',\n",
       "  'greet',\n",
       "  'others',\n",
       "  'namaste',\n",
       "  'customary',\n",
       "  'respectful',\n",
       "  'indian',\n",
       "  'greeting',\n",
       "  'also',\n",
       "  'part',\n",
       "  'atmantan',\n",
       "  'culture'],\n",
       " ['lunch',\n",
       "  'today',\n",
       "  'daughter',\n",
       "  'jacket',\n",
       "  'potato',\n",
       "  'tuna',\n",
       "  'cheese',\n",
       "  'hea',\n",
       "  'speedy',\n",
       "  'salad'],\n",
       " ['whenever',\n",
       "  'faced',\n",
       "  'fear',\n",
       "  'always',\n",
       "  'choice',\n",
       "  'fear',\n",
       "  'something',\n",
       "  'faced',\n",
       "  'face',\n",
       "  'inevitable',\n",
       "  'something',\n",
       "  'need',\n",
       "  'change',\n",
       "  'let',\n",
       "  'fear',\n",
       "  'win',\n",
       "  'power',\n",
       "  'believe',\n",
       "  'strong',\n",
       "  'capable',\n",
       "  'forget',\n",
       "  'everything',\n",
       "  'run',\n",
       "  'face',\n",
       "  'everything',\n",
       "  'rise',\n",
       "  'choice'],\n",
       " ['love',\n",
       "  'cucumber',\n",
       "  'high',\n",
       "  'vitamin',\n",
       "  'c',\n",
       "  'vitamin',\n",
       "  'k',\n",
       "  'contains',\n",
       "  'lot',\n",
       "  'antioxidant',\n",
       "  'block',\n",
       "  'oxidation',\n",
       "  'meaning',\n",
       "  'protects',\n",
       "  'skin',\n",
       "  'free',\n",
       "  'radical',\n",
       "  'air',\n",
       "  'keep',\n",
       "  'skin',\n",
       "  'looking',\n",
       "  'youthful',\n",
       "  'little',\n",
       "  'beauty',\n",
       "  'also',\n",
       "  'reduce',\n",
       "  'risk',\n",
       "  'chronic',\n",
       "  'disease',\n",
       "  'reduces',\n",
       "  'blood',\n",
       "  'sugar',\n",
       "  'promotes',\n",
       "  'hydration',\n",
       "  'green',\n",
       "  'ha',\n",
       "  'green',\n",
       "  'apple',\n",
       "  'pineapple',\n",
       "  'kale',\n",
       "  'cucumber',\n",
       "  'thing',\n",
       "  'need',\n",
       "  'supercharge',\n",
       "  'day'],\n",
       " ['gift',\n",
       "  'time',\n",
       "  'swipe',\n",
       "  'always',\n",
       "  'go',\n",
       "  'revivesuperfoods',\n",
       "  'ha',\n",
       "  'saved',\n",
       "  'spend',\n",
       "  'time',\n",
       "  'making',\n",
       "  'healthy',\n",
       "  'breakfast',\n",
       "  'every',\n",
       "  'morning',\n",
       "  'ready',\n",
       "  'blend',\n",
       "  'smoothy',\n",
       "  'crafted',\n",
       "  'farm',\n",
       "  'frozen',\n",
       "  'premium',\n",
       "  'ingredient',\n",
       "  'delivered',\n",
       "  'right',\n",
       "  'door',\n",
       "  'add',\n",
       "  'liquid',\n",
       "  'blend',\n",
       "  'tried',\n",
       "  'flavour',\n",
       "  'love',\n",
       "  'revivesuperfoods',\n",
       "  'wa',\n",
       "  'awesome',\n",
       "  'enough',\n",
       "  'partner',\n",
       "  'hosting',\n",
       "  'lucky',\n",
       "  'winner',\n",
       "  'randomly',\n",
       "  'selected',\n",
       "  'win',\n",
       "  'box',\n",
       "  'best',\n",
       "  'selling',\n",
       "  'smoothy',\n",
       "  'overnight',\n",
       "  'oat',\n",
       "  'enter',\n",
       "  'follow',\n",
       "  'revivesuperfoods',\n",
       "  'tag',\n",
       "  'friend',\n",
       "  'sharing',\n",
       "  'prize',\n",
       "  'let',\n",
       "  'know',\n",
       "  'comment',\n",
       "  'fav',\n",
       "  'smoothie',\n",
       "  'ingredient',\n",
       "  'use',\n",
       "  'code',\n",
       "  'get',\n",
       "  'giveaway',\n",
       "  'open',\n",
       "  'u',\n",
       "  'canadian',\n",
       "  'resident',\n",
       "  'winner',\n",
       "  'chosen',\n",
       "  'day',\n",
       "  'goodluck'],\n",
       " ['scared',\n",
       "  'coronavirus',\n",
       "  'please',\n",
       "  'worry',\n",
       "  'take',\n",
       "  'safety',\n",
       "  'measure',\n",
       "  'remember',\n",
       "  'one',\n",
       "  'day',\n",
       "  'everyone',\n",
       "  'ha',\n",
       "  'die',\n",
       "  'one',\n",
       "  'day',\n",
       "  'anyday',\n",
       "  'everyone',\n",
       "  'else',\n",
       "  'planet',\n",
       "  'take',\n",
       "  'today',\n",
       "  'granted',\n",
       "  'try',\n",
       "  'leave',\n",
       "  'regret',\n",
       "  'recall',\n",
       "  'deathbed',\n",
       "  'spend',\n",
       "  'everyday',\n",
       "  'gift',\n",
       "  'drop',\n",
       "  'agree',\n",
       "  'share',\n",
       "  'spread',\n",
       "  'awareness',\n",
       "  'save',\n",
       "  'post',\n",
       "  'later',\n",
       "  'check',\n",
       "  'happysoulplus',\n",
       "  'find',\n",
       "  'soulful',\n",
       "  'thought'],\n",
       " ['love',\n",
       "  'mani',\n",
       "  'pedis',\n",
       "  'doe',\n",
       "  'help',\n",
       "  'u',\n",
       "  'feel',\n",
       "  'pampered',\n",
       "  'also',\n",
       "  'make',\n",
       "  'u',\n",
       "  'feel',\n",
       "  'confident',\n",
       "  'however',\n",
       "  'also',\n",
       "  'aware',\n",
       "  'risk',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'overall',\n",
       "  'health',\n",
       "  'infection',\n",
       "  'contact',\n",
       "  'dermatitis',\n",
       "  'plantar',\n",
       "  'wart',\n",
       "  'cancer',\n",
       "  'aging',\n",
       "  'complication',\n",
       "  'possibly',\n",
       "  'caused',\n",
       "  'tool',\n",
       "  'disinfected',\n",
       "  'sanitized',\n",
       "  'general',\n",
       "  'uncleanliness',\n",
       "  'salon',\n",
       "  'whole',\n",
       "  'poor',\n",
       "  'hygiene',\n",
       "  'practice',\n",
       "  'improper',\n",
       "  'cleaning',\n",
       "  'foot',\n",
       "  'bath',\n",
       "  'nail',\n",
       "  'technician',\n",
       "  'may',\n",
       "  'aggressive',\n",
       "  'technician',\n",
       "  'may',\n",
       "  'careless',\n",
       "  'inattentive',\n",
       "  'read',\n",
       "  'site',\n",
       "  'link',\n",
       "  'bio'],\n",
       " ['know',\n",
       "  'morning',\n",
       "  'person',\n",
       "  'give',\n",
       "  'coffee',\n",
       "  'day',\n",
       "  'everyday',\n",
       "  'working',\n",
       "  'mom',\n",
       "  'struggle',\n",
       "  'stay',\n",
       "  'awake',\n",
       "  'sometimes',\n",
       "  'recently',\n",
       "  'found',\n",
       "  'uplift',\n",
       "  'energizing',\n",
       "  'body',\n",
       "  'cream',\n",
       "  'theseaweedbathco',\n",
       "  'available',\n",
       "  'wholefoods',\n",
       "  'let',\n",
       "  'tell',\n",
       "  'amazing',\n",
       "  'doe',\n",
       "  'vitamin',\n",
       "  'c',\n",
       "  'help',\n",
       "  'replenish',\n",
       "  'protect',\n",
       "  'skin',\n",
       "  'also',\n",
       "  'ha',\n",
       "  'matcha',\n",
       "  'ginseng',\n",
       "  'coffee',\n",
       "  'bean',\n",
       "  'extract',\n",
       "  'energize',\n",
       "  'invigorate',\n",
       "  'skin',\n",
       "  'anything',\n",
       "  'coffee',\n",
       "  'win',\n",
       "  'book',\n",
       "  'gon',\n",
       "  'na',\n",
       "  'lie',\n",
       "  'grapefruit',\n",
       "  'orange',\n",
       "  'scent',\n",
       "  'good',\n",
       "  'pouring',\n",
       "  'second',\n",
       "  'maybe',\n",
       "  'third',\n",
       "  'cup',\n",
       "  'coffee',\n",
       "  'today',\n",
       "  'using',\n",
       "  'new',\n",
       "  'body',\n",
       "  'cream',\n",
       "  'help',\n",
       "  'stay',\n",
       "  'energized',\n",
       "  'unique',\n",
       "  'way',\n",
       "  'found',\n",
       "  'help',\n",
       "  'stay',\n",
       "  'energized']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = instagram.text_processed.tolist()\n",
    "data[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['best', 'offense', 'good', 'defense', 'healthy', 'tip', 'boast', 'immunity', 'work', 'covid', 'scare', 'scared', 'take', 'care']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words])\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good', 'good', 'defense', 'healthy', 'immunity', 'work', 'scare', 'take', 'care']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('care', 1),\n",
       "  ('defense', 1),\n",
       "  ('good', 2),\n",
       "  ('healthy', 1),\n",
       "  ('immunity', 1),\n",
       "  ('scare', 1),\n",
       "  ('take', 1),\n",
       "  ('work', 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term frequency\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.135*\"plan\" + 0.127*\"meal\" + 0.086*\"case\" + 0.058*\"snack\" + 0.049*\"super\" + 0.037*\"listen\" + 0.034*\"throw\" + 0.029*\"death\" + 0.028*\"inflammatory\" + 0.025*\"lack\"'),\n",
       " (1,\n",
       "  '0.133*\"safe\" + 0.084*\"rid\" + 0.077*\"must\" + 0.069*\"avoid\" + 0.044*\"practice\" + 0.043*\"burn\" + 0.040*\"topic\" + 0.034*\"young\" + 0.034*\"heat\" + 0.026*\"human\"'),\n",
       " (2,\n",
       "  '0.147*\"mix\" + 0.141*\"cook\" + 0.062*\"egg\" + 0.041*\"roll\" + 0.034*\"inch\" + 0.010*\"splash\" + 0.002*\"lime\" + 0.000*\"slice\" + 0.000*\"soup\" + 0.000*\"sauce\"'),\n",
       " (3,\n",
       "  '0.124*\"find\" + 0.079*\"call\" + 0.065*\"challenge\" + 0.043*\"stop\" + 0.040*\"leave\" + 0.033*\"list\" + 0.032*\"able\" + 0.032*\"habit\" + 0.032*\"client\" + 0.029*\"daily\"'),\n",
       " (4,\n",
       "  '0.084*\"painful\" + 0.075*\"fear\" + 0.064*\"capable\" + 0.050*\"battle\" + 0.048*\"kill\" + 0.048*\"power\" + 0.022*\"scary\" + 0.000*\"woman\" + 0.000*\"fibroid\" + 0.000*\"parent\"'),\n",
       " (5,\n",
       "  '0.122*\"pain\" + 0.108*\"say\" + 0.100*\"tell\" + 0.071*\"sometimes\" + 0.051*\"heart\" + 0.045*\"other\" + 0.039*\"remember\" + 0.035*\"maybe\" + 0.035*\"away\" + 0.032*\"matter\"'),\n",
       " (6,\n",
       "  '0.082*\"eat\" + 0.065*\"food\" + 0.049*\"often\" + 0.040*\"exercise\" + 0.040*\"cause\" + 0.040*\"may\" + 0.036*\"diet\" + 0.033*\"important\" + 0.032*\"include\" + 0.023*\"also\"'),\n",
       " (7,\n",
       "  '0.195*\"weight\" + 0.102*\"program\" + 0.071*\"calorie\" + 0.045*\"short\" + 0.038*\"sign\" + 0.038*\"soon\" + 0.032*\"bed\" + 0.026*\"reminder\" + 0.025*\"regularly\" + 0.025*\"forget\"'),\n",
       " (8,\n",
       "  '0.118*\"water\" + 0.096*\"add\" + 0.088*\"morning\" + 0.077*\"product\" + 0.058*\"breakfast\" + 0.041*\"natural\" + 0.040*\"skin\" + 0.036*\"blend\" + 0.029*\"second\" + 0.028*\"open\"'),\n",
       " (9,\n",
       "  '0.251*\"goal\" + 0.111*\"set\" + 0.059*\"website\" + 0.044*\"achieve\" + 0.037*\"dream\" + 0.037*\"limit\" + 0.032*\"green\" + 0.032*\"event\" + 0.031*\"constantly\" + 0.030*\"plant\"'),\n",
       " (10,\n",
       "  '0.316*\"lifestyle\" + 0.156*\"reduce\" + 0.097*\"present\" + 0.056*\"class\" + 0.047*\"rate\" + 0.020*\"weight_loss\" + 0.019*\"pineapple\" + 0.016*\"cucumber\" + 0.012*\"beauty\" + 0.008*\"valuable\"'),\n",
       " (11,\n",
       "  '0.114*\"home\" + 0.105*\"possible\" + 0.046*\"book\" + 0.045*\"coffee\" + 0.044*\"shake\" + 0.043*\"meet\" + 0.034*\"ice\" + 0.028*\"allergy\" + 0.028*\"flavor\" + 0.026*\"cream\"'),\n",
       " (12,\n",
       "  '0.055*\"high\" + 0.052*\"level\" + 0.052*\"perform\" + 0.047*\"also\" + 0.045*\"low\" + 0.043*\"treatment\" + 0.039*\"fat\" + 0.037*\"increase\" + 0.034*\"benefit\" + 0.027*\"improve\"'),\n",
       " (13,\n",
       "  '0.082*\"back\" + 0.057*\"result\" + 0.055*\"first\" + 0.055*\"month\" + 0.054*\"story\" + 0.052*\"lose\" + 0.034*\"together\" + 0.032*\"begin\" + 0.031*\"drop\" + 0.030*\"ask\"'),\n",
       " (14,\n",
       "  '0.149*\"enjoy\" + 0.055*\"walk\" + 0.055*\"hope\" + 0.042*\"warm\" + 0.038*\"definitely\" + 0.037*\"dinner\" + 0.035*\"hit\" + 0.034*\"tonight\" + 0.033*\"lunch\" + 0.031*\"hormone\"'),\n",
       " (15,\n",
       "  '0.153*\"family\" + 0.126*\"world\" + 0.100*\"relax\" + 0.097*\"order\" + 0.077*\"light\" + 0.060*\"research\" + 0.051*\"fresh\" + 0.042*\"awesome\" + 0.015*\"sudden\" + 0.008*\"capsule\"'),\n",
       " (16,\n",
       "  '0.107*\"grow\" + 0.089*\"growth\" + 0.065*\"eye\" + 0.063*\"actually\" + 0.062*\"question\" + 0.057*\"affect\" + 0.040*\"miss\" + 0.038*\"moment\" + 0.037*\"write\" + 0.031*\"realize\"'),\n",
       " (17,\n",
       "  '0.153*\"body\" + 0.120*\"symptom\" + 0.049*\"free\" + 0.040*\"age\" + 0.031*\"typically\" + 0.028*\"show\" + 0.028*\"gym\" + 0.028*\"move\" + 0.025*\"picture\" + 0.023*\"run\"'),\n",
       " (18,\n",
       "  '0.128*\"protein\" + 0.121*\"example\" + 0.093*\"info\" + 0.093*\"build\" + 0.048*\"busy\" + 0.047*\"session\" + 0.032*\"bean\" + 0.029*\"acid\" + 0.027*\"truth\" + 0.024*\"member\"'),\n",
       " (19,\n",
       "  '0.033*\"make\" + 0.026*\"day\" + 0.025*\"time\" + 0.025*\"get\" + 0.022*\"want\" + 0.022*\"healthy\" + 0.021*\"go\" + 0.021*\"know\" + 0.020*\"help\" + 0.019*\"take\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "lda_model.print_topics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics still too \"dispersed\". Will use other paremeters to find more \"solid\" topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model2 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model2.print_topics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With 5 topics:\n",
    "- body\n",
    "- disease (fibroid)\n",
    "- diet advice / what to eat and what not\n",
    "- positive / action verbs e.g. make, go, etc\n",
    "- workout / program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning trying different passes, etc\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 topics\n",
    "- body, help with body issues? food and exercise\n",
    "- disease\n",
    "- recipes\n",
    "- action verbs\n",
    "- workout program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping nouns only\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nouns = instagram[\"post\"].apply(str).apply(nouns)\n",
    "data_nouns[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_nouns)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics if only accounting for nouns (above)\n",
    "- food, female topics (period, estrogen)\n",
    "- woman, leadership\n",
    "- recipe?\n",
    "- disease, medical procedures\n",
    "- symptom/pain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing chunksize\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=50,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics when chunkside is reduced (above)\n",
    "- procedure, treatment\n",
    "- woman, life\n",
    "- fruit, plann\n",
    "- disease, symptom\n",
    "- day, food\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunkside back to 100, passes 50, 4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=50,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 4 topics\n",
    "- woman, food, weight, ageing\n",
    "- day, year, goal - probably resolutions\n",
    "- ?\n",
    "- disease, procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 topics\n",
    "lda_model3 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=50,\n",
    "                                           passes=50,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"make\" + 0.013*\"time\" + 0.012*\"day\" + 0.012*\"get\" + 0.011*\"want\" + 0.010*\"go\" + 0.010*\"know\" + 0.010*\"healthy\" + 0.010*\"take\" + 0.008*\"feel\"'),\n",
       " (1,\n",
       "  '0.044*\"fibroid\" + 0.027*\"woman\" + 0.017*\"patient\" + 0.016*\"symptom\" + 0.012*\"procedure\" + 0.012*\"center\" + 0.011*\"ufe\" + 0.010*\"surgery\" + 0.010*\"cause\" + 0.009*\"adenomyosis\"'),\n",
       " (2,\n",
       "  '0.015*\"water\" + 0.012*\"add\" + 0.010*\"breakfast\" + 0.010*\"use\" + 0.009*\"food\" + 0.009*\"drink\" + 0.008*\"eat\" + 0.008*\"oil\" + 0.008*\"high\" + 0.008*\"protein\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model3.print_topics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 topics (above)\n",
    "\n",
    "- diet/food: water, recipe, protein\n",
    "- resolution: the healthy life, decisions, woman\n",
    "- diseases: fibroid, woman, procedure, treatment, pain, opinion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Afer experimenting with different parameters, 3 topics seems to give us a solid idea of the different existent topics founds in the instagram posts. \n",
    "This might need to be changed/ further \"tuned\" if more data becomes available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.ldamodel.LdaModel"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lda_model3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
